{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with Keras (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This builds on the previous notebook, where \n",
    "\n",
    "1) the bottleneck features of VGG16 on our images were recorded and stored\n",
    "\n",
    "2) a top level classifier was trained on top of those features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we finally build a unified model and additionally, try fine tuning the last convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1) instantiate the convolutional base of VGG16 and load its weights\n",
    "\n",
    "2) add our previously defined fully-connected model on top, and load its weights\n",
    "\n",
    "3) freeze the layers of the VGG16 model up to the last convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 500,375\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "test_data_dir = 'data/test'\n",
    "\n",
    "n_train_samples = 235\n",
    "n_train_ants = 114\n",
    "n_train_bees = 121\n",
    "\n",
    "n_test_samples = 148\n",
    "n_test_ants = 66\n",
    "n_test_bees = 82\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "final_model_weights_path = \"final_model.h5\"\n",
    "\n",
    "model_exists = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load convolutional layers from VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 375, 500, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 375, 500, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 375, 500, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 187, 250, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 187, 250, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 187, 250, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 93, 125, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 93, 125, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 46, 62, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 46, 62, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 23, 31, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 11, 15, 512)       0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network\n",
    "base_model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width,3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add classifier on top, using the saved weights from the training before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 84480)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               21627136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 21,627,393\n",
      "Trainable params: 21,627,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# note that it is necessary to start with a fully-trained # classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build unified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 375, 500, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 375, 500, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 375, 500, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 187, 250, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 187, 250, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 187, 250, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 93, 125, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 93, 125, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 46, 62, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 46, 62, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 23, 31, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 11, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 21627393  \n",
      "=================================================================\n",
      "Total params: 36,342,081\n",
      "Trainable params: 36,342,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### freeze all layers before last conv block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the first 15 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tune model using very small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 235 images belonging to 2 classes.\n",
      "Found 148 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists:\n",
    "    # fine-tune the model\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=n_test_samples // batch_size,\n",
    "        verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not model_exists:\n",
    "    model.save(final_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 375, 500, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 375, 500, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 375, 500, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 187, 250, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 187, 250, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 187, 250, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 93, 125, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 93, 125, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 93, 125, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 46, 62, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 46, 62, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 46, 62, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 23, 31, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 23, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 11, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 21627393  \n",
      "=================================================================\n",
      "Total params: 36,342,081\n",
      "Trainable params: 28,706,817\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if model_exists:\n",
    "    model = load_model(final_model_weights_path)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path = 'data/train/bees/1092977343_cb42b38d62.jpg'\n",
    "img = load_img(test_img_path) \n",
    "candidate = np.expand_dims(img_to_array(img)/255, axis=0)\n",
    "model.predict(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_img(img_path):\n",
    "    img = load_img(img_path) \n",
    "    candidate = np.expand_dims(img_to_array(img)/255, axis=0)\n",
    "    pred = model.predict(candidate, verbose=0)\n",
    "    pred_class = (pred > 0.5).astype('int32')\n",
    "    print(\"class: {}    prob: {}    pic: {}\".format(pred_class, pred, img_path))\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: [[0]]    prob: [[  3.85120700e-07]]    pic: data/test/ants/2238242353_52c82441df.jpg\n",
      "class: [[0]]    prob: [[ 0.00010975]]    pic: data/test/ants/161292361_c16e0bf57a.jpg\n",
      "class: [[0]]    prob: [[  1.36585241e-08]]    pic: data/test/ants/35558229_1fa4608a7a.jpg\n",
      "class: [[0]]    prob: [[  1.46376664e-07]]    pic: data/test/ants/445356866_6cb3289067.jpg\n",
      "class: [[0]]    prob: [[ 0.0013961]]    pic: data/test/ants/2255445811_dabcdf7258.jpg\n",
      "class: [[0]]    prob: [[  1.78326570e-06]]    pic: data/test/ants/147542264_79506478c2.jpg\n",
      "class: [[0]]    prob: [[  1.17502452e-08]]    pic: data/test/ants/518746016_bcc28f8b5b.jpg\n",
      "class: [[0]]    prob: [[ 0.00813808]]    pic: data/test/ants/1743840368_b5ccda82b7.jpg\n",
      "class: [[0]]    prob: [[  1.08645262e-08]]    pic: data/test/ants/94999827_36895faade.jpg\n",
      "class: [[0]]    prob: [[ 0.06374494]]    pic: data/test/ants/854534770_31f6156383.jpg\n",
      "class: [[0]]    prob: [[  2.94871688e-06]]    pic: data/test/ants/152286280_411648ec27.jpg\n",
      "class: [[0]]    prob: [[  5.01354736e-09]]    pic: data/test/ants/1337725712_2eb53cd742.jpg\n",
      "class: [[0]]    prob: [[  3.12333839e-08]]    pic: data/test/ants/205398178_c395c5e460.jpg\n",
      "class: [[0]]    prob: [[  3.29932109e-10]]    pic: data/test/ants/1124525276_816a07c17f.jpg\n",
      "class: [[0]]    prob: [[  6.05797050e-06]]    pic: data/test/ants/2219621907_47bc7cc6b0.jpg\n",
      "class: [[0]]    prob: [[  2.01168145e-11]]    pic: data/test/ants/153783656_85f9c3ac70.jpg\n",
      "class: [[0]]    prob: [[  5.22143091e-05]]    pic: data/test/ants/212100470_b485e7b7b9.jpg\n",
      "class: [[0]]    prob: [[  3.10544750e-11]]    pic: data/test/ants/239161491_86ac23b0a3.jpg\n",
      "class: [[0]]    prob: [[  9.78789649e-06]]    pic: data/test/ants/459442412_412fecf3fe.jpg\n",
      "class: [[0]]    prob: [[ 0.00012159]]    pic: data/test/ants/F.pergan.28(f).jpg\n",
      "class: [[1]]    prob: [[ 0.99311453]]    pic: data/test/ants/161076144_124db762d6.jpg\n",
      "class: [[0]]    prob: [[  2.03364743e-06]]    pic: data/test/ants/209615353_eeb38ba204.jpg\n",
      "class: [[0]]    prob: [[  9.99363294e-08]]    pic: data/test/ants/1440002809_b268d9a66a.jpg\n",
      "class: [[0]]    prob: [[ 0.00391035]]    pic: data/test/ants/2191997003_379df31291.jpg\n",
      "class: [[0]]    prob: [[ 0.02490323]]    pic: data/test/ants/649407494_9b6bc4949f.jpg\n",
      "class: [[0]]    prob: [[  1.09838579e-11]]    pic: data/test/ants/1073564163_225a64f170.jpg\n",
      "class: [[0]]    prob: [[  9.99475469e-06]]    pic: data/test/ants/540543309_ddbb193ee5.jpg\n",
      "class: [[0]]    prob: [[ 0.00011874]]    pic: data/test/ants/477437164_bc3e6e594a.jpg\n",
      "class: [[0]]    prob: [[  1.24627729e-06]]    pic: data/test/ants/208072188_f293096296.jpg\n",
      "class: [[0]]    prob: [[  1.49217503e-05]]    pic: data/test/ants/153320619_2aeb5fa0ee.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/ants/8398478_50ef10c47a.jpg\n",
      "class: [[0]]    prob: [[ 0.01326572]]    pic: data/test/ants/10308379_1b6c72e180.jpg\n",
      "class: [[0]]    prob: [[ 0.00908924]]    pic: data/test/ants/159515240_d5981e20d1.jpg\n",
      "class: [[0]]    prob: [[ 0.00823667]]    pic: data/test/ants/562589509_7e55469b97.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/ants/892676922_4ab37dce07.jpg\n",
      "class: [[0]]    prob: [[ 0.00040907]]    pic: data/test/ants/1053149811_f62a3410d3.jpg\n",
      "class: [[1]]    prob: [[ 0.99290317]]    pic: data/test/ants/2127908701_d49dc83c97.jpg\n",
      "class: [[1]]    prob: [[ 0.98693013]]    pic: data/test/ants/170652283_ecdaff5d1a.jpg\n",
      "class: [[0]]    prob: [[ 0.00891226]]    pic: data/test/ants/2104709400_8831b4fc6f.jpg\n",
      "class: [[0]]    prob: [[ 0.0002759]]    pic: data/test/ants/436944325_d4925a38c7.jpg\n",
      "class: [[0]]    prob: [[  2.25599833e-05]]    pic: data/test/ants/17081114_79b9a27724.jpg\n",
      "class: [[0]]    prob: [[ 0.06246308]]    pic: data/test/ants/1247887232_edcb61246c.jpg\n",
      "class: [[0]]    prob: [[  5.77290535e-08]]    pic: data/test/ants/Ant-1818.jpg\n",
      "class: [[0]]    prob: [[  9.21330923e-07]]    pic: data/test/ants/768870506_8f115d3d37.jpg\n",
      "class: [[0]]    prob: [[  6.23138403e-05]]    pic: data/test/ants/1358854066_5ad8015f7f.jpg\n",
      "class: [[1]]    prob: [[ 0.99470079]]    pic: data/test/ants/751649788_78dd7d16ce.jpg\n",
      "class: [[0]]    prob: [[ 0.32137236]]    pic: data/test/ants/308196310_1db5ffa01b.jpg\n",
      "class: [[0]]    prob: [[  4.25610231e-13]]    pic: data/test/ants/488272201_c5aa281348.jpg\n",
      "class: [[1]]    prob: [[ 0.94900036]]    pic: data/test/ants/319494379_648fb5a1c6.jpg\n",
      "class: [[0]]    prob: [[  2.61271360e-09]]    pic: data/test/ants/263615709_cfb28f6b8e.jpg\n",
      "class: [[0]]    prob: [[ 0.0002715]]    pic: data/test/ants/573151833_ebbc274b77.jpg\n",
      "class: [[0]]    prob: [[ 0.00047836]]    pic: data/test/ants/11381045_b352a47d8c.jpg\n",
      "class: [[0]]    prob: [[  2.24058283e-07]]    pic: data/test/ants/1119630822_cd325ea21a.jpg\n",
      "class: [[0]]    prob: [[  2.74165759e-05]]    pic: data/test/ants/172772109_d0a8e15fb0.jpg\n",
      "class: [[0]]    prob: [[  2.91163804e-08]]    pic: data/test/ants/2211974567_ee4606b493.jpg\n",
      "class: [[0]]    prob: [[  7.45361021e-13]]    pic: data/test/ants/412436937_4c2378efc2.jpg\n",
      "class: [[0]]    prob: [[  1.06850043e-10]]    pic: data/test/ants/502717153_3e4865621a.jpg\n",
      "class: [[0]]    prob: [[ 0.01628936]]    pic: data/test/ants/157401988_d0564a9d02.jpg\n",
      "class: [[0]]    prob: [[  2.08640522e-05]]    pic: data/test/ants/1262751255_c56c042b7b.jpg\n",
      "class: [[0]]    prob: [[  2.19066312e-08]]    pic: data/test/ants/181942028_961261ef48.jpg\n",
      "class: [[0]]    prob: [[ 0.03307979]]    pic: data/test/ants/183260961_64ab754c97.jpg\n",
      "class: [[0]]    prob: [[  3.42877388e-06]]    pic: data/test/ants/470127071_8b8ee2bd74.jpg\n",
      "class: [[0]]    prob: [[ 0.34929481]]    pic: data/test/ants/119785936_dd428e40c3.jpg\n",
      "class: [[0]]    prob: [[ 0.00066634]]    pic: data/test/ants/8124241_36b290d372.jpg\n",
      "class: [[0]]    prob: [[ 0.00155667]]    pic: data/test/ants/57264437_a19006872f.jpg\n",
      "class: [[0]]    prob: [[  1.90486639e-06]]    pic: data/test/ants/2039585088_c6f47c592e.jpg\n",
      "Detected ant in 59 of 66 images\n"
     ]
    }
   ],
   "source": [
    "num_ants_found = 0\n",
    "ants_dir = os.path.join(test_data_dir, \"ants\")\n",
    "for imgfile in os.listdir(ants_dir):\n",
    "    score = score_img(os.path.join(ants_dir, imgfile))\n",
    "    if score == 0: num_ants_found = num_ants_found + 1\n",
    "print('Detected ant in {} of {} images'.format(num_ants_found, n_test_ants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: [[0]]    prob: [[ 0.00024186]]    pic: data/test/bees/485743562_d8cc6b8f73.jpg\n",
      "class: [[1]]    prob: [[ 0.58852655]]    pic: data/test/bees/290082189_f66cb80bfc.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/1355974687_1341c1face.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2415414155_1916f03b42.jpg\n",
      "class: [[1]]    prob: [[ 0.99875963]]    pic: data/test/bees/456097971_860949c4fc.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/1799729694_0c40101071.jpg\n",
      "class: [[1]]    prob: [[ 0.99999404]]    pic: data/test/bees/59798110_2b6a3c8031.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2457841282_7867f16639.jpg\n",
      "class: [[1]]    prob: [[ 0.97857863]]    pic: data/test/bees/1032546534_06907fe3b3.jpg\n",
      "class: [[0]]    prob: [[ 0.00011714]]    pic: data/test/bees/1519368889_4270261ee3.jpg\n",
      "class: [[0]]    prob: [[ 0.11086833]]    pic: data/test/bees/2509402554_31821cb0b6.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/72100438_73de9f17af.jpg\n",
      "class: [[0]]    prob: [[ 0.13257106]]    pic: data/test/bees/2709775832_85b4b50a57.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2060668999_e11edb10d0.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/272986700_d4d4bf8c4b.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2883093452_7e3a1eb53f.jpg\n",
      "class: [[0]]    prob: [[ 0.01223302]]    pic: data/test/bees/353266603_d3eac7e9a0.jpg\n",
      "class: [[1]]    prob: [[ 0.99999964]]    pic: data/test/bees/1328423762_f7a88a8451.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/348291597_ee836fbb1a.jpg\n",
      "class: [[1]]    prob: [[ 0.99981445]]    pic: data/test/bees/abeja.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/57459255_752774f1b2.jpg\n",
      "class: [[0]]    prob: [[  1.22765584e-08]]    pic: data/test/bees/2470492902_3572c90f75.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/590318879_68cf112861.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2407809945_fb525ef54d.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/144098310_a4176fd54d.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/187130242_4593a4c610.jpg\n",
      "class: [[0]]    prob: [[ 0.00014629]]    pic: data/test/bees/54736755_c057723f64.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2841437312_789699c740.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2741763055_9a7bb00802.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/296565463_d07a7bed96.jpg\n",
      "class: [[1]]    prob: [[ 0.7924394]]    pic: data/test/bees/1486120850_490388f84b.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2745389517_250a397f31.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2751836205_6f7b5eff30.jpg\n",
      "class: [[1]]    prob: [[ 0.99854362]]    pic: data/test/bees/2668391343_45e272cd07.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2685605303_9eed79d59d.jpg\n",
      "class: [[1]]    prob: [[ 0.62313795]]    pic: data/test/bees/2104135106_a65eede1de.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/1297972485_33266a18d9.jpg\n",
      "class: [[1]]    prob: [[ 0.99991846]]    pic: data/test/bees/220376539_20567395d8.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/215512424_687e1e0821.jpg\n",
      "class: [[1]]    prob: [[ 0.88115585]]    pic: data/test/bees/2670536155_c170f49cd0.jpg\n",
      "class: [[1]]    prob: [[ 0.64375234]]    pic: data/test/bees/464594019_1b24a28bb1.jpg\n",
      "class: [[1]]    prob: [[ 0.99966884]]    pic: data/test/bees/2478216347_535c8fe6d7.jpg\n",
      "class: [[1]]    prob: [[ 0.8800233]]    pic: data/test/bees/2501530886_e20952b97d.jpg\n",
      "class: [[1]]    prob: [[ 0.95815897]]    pic: data/test/bees/586474709_ae436da045.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/936182217_c4caa5222d.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2506114833_90a41c5267.jpg\n",
      "class: [[1]]    prob: [[ 0.98397702]]    pic: data/test/bees/603709866_a97c7cfc72.jpg\n",
      "class: [[1]]    prob: [[ 0.87336576]]    pic: data/test/bees/603711658_4c8cd2201e.jpg\n",
      "class: [[1]]    prob: [[ 0.99991584]]    pic: data/test/bees/2438480600_40a1249879.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/372228424_16da1f8884.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/181171681_c5a1a82ded.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2815838190_0a9889d995.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/151594775_ee7dc17b60.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2086294791_6f3789d8a6.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/540976476_844950623f.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2702408468_d9ed795f4f.jpg\n",
      "class: [[1]]    prob: [[ 0.99996305]]    pic: data/test/bees/26589803_5ba7000313.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/44105569_16720a960c.jpg\n",
      "class: [[1]]    prob: [[ 0.99999964]]    pic: data/test/bees/151603988_2c6f7d14c7.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2173503984_9c6aaaa7e2.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/65038344_52a45d090d.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2782079948_8d4e94a826.jpg\n",
      "class: [[1]]    prob: [[ 0.9858833]]    pic: data/test/bees/224841383_d050f5f510.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/149973093_da3c446268.jpg\n",
      "class: [[1]]    prob: [[ 0.99994385]]    pic: data/test/bees/238161922_55fa9a76ae.jpg\n",
      "class: [[0]]    prob: [[ 0.05566247]]    pic: data/test/bees/759745145_e8bc776ec8.jpg\n",
      "class: [[1]]    prob: [[ 0.99993885]]    pic: data/test/bees/350436573_41f4ecb6c8.jpg\n",
      "class: [[0]]    prob: [[  7.43570636e-05]]    pic: data/test/bees/416144384_961c326481.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/3077452620_548c79fda0.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2809496124_5f25b5946a.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/203868383_0fcbb48278.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/152789693_220b003452.jpg\n",
      "class: [[1]]    prob: [[ 0.99999726]]    pic: data/test/bees/10870992_eebeeb3a12.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2321144482_f3785ba7b2.jpg\n",
      "class: [[1]]    prob: [[ 0.99999917]]    pic: data/test/bees/2103637821_8d26ee6b90.jpg\n",
      "class: [[1]]    prob: [[ 0.99998355]]    pic: data/test/bees/400262091_701c00031c.jpg\n",
      "class: [[1]]    prob: [[ 0.99999952]]    pic: data/test/bees/2444778727_4b781ac424.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/2717418782_bd83307d9f.jpg\n",
      "class: [[1]]    prob: [[ 0.65316397]]    pic: data/test/bees/576452297_897023f002.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/177677657_a38c97e572.jpg\n",
      "class: [[1]]    prob: [[ 0.99999988]]    pic: data/test/bees/2525379273_dcb26a516d.jpg\n",
      "class: [[1]]    prob: [[ 1.]]    pic: data/test/bees/1181173278_23c36fac71.jpg\n",
      "Detected bee in 73 of 82 images\n"
     ]
    }
   ],
   "source": [
    "num_bees_found = 0\n",
    "bees_dir = os.path.join(test_data_dir, \"bees\")\n",
    "for imgfile in os.listdir(bees_dir):\n",
    "    score = score_img(os.path.join(bees_dir, imgfile))\n",
    "    if score == 1: num_bees_found = num_bees_found + 1\n",
    "print('Detected bee in {} of {} images'.format(num_bees_found, n_test_bees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.892, sensitivity: 0.894, precision: 0.868\n"
     ]
    }
   ],
   "source": [
    "# let's say ants are positive\n",
    "true_positives = num_ants_found\n",
    "false_negatives = n_test_ants - true_positives\n",
    "true_negatives = num_bees_found\n",
    "false_positives = n_test_bees - true_negatives\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (n_test_ants + n_test_bees)\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "print('Accuracy: {:.3f}, sensitivity: {:.3f}, precision: {:.3f}'.format(accuracy, sensitivity, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
