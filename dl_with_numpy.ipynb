{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep neural network with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic behind deep neural nets is backpropagation.\n",
    "To see that there is no rocket science in simple backprop, we are going to build a neural network with NumPy alone, thus implementing backpropagation ourselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define basic dimensions of our network\n",
    "n = 64\n",
    "num_features = 1000\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1000), (64, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input data\n",
    "X = np.random.randn(n, num_features)           # 64 * 1000\n",
    "y = np.random.randn(n, output_dim)             # 64 * 10\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "W1 = np.random.randn(num_features, hidden_dim) # 1000 * 100\n",
    "W2 = np.random.randn(hidden_dim, output_dim)   # 100 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36039354.2859\n",
      "10 1107377.86887\n",
      "20 223788.831893\n",
      "30 74448.3927778\n",
      "40 30588.5287447\n",
      "50 14548.0852616\n",
      "60 7640.99248799\n",
      "70 4311.31890829\n",
      "80 2564.0782734\n",
      "90 1585.08298479\n",
      "100 1008.53639513\n",
      "110 655.691456916\n",
      "120 433.35125501\n",
      "130 290.085479884\n",
      "140 196.249841286\n",
      "150 133.904862291\n",
      "160 91.982364619\n",
      "170 63.5307245658\n",
      "180 44.0838019396\n",
      "190 30.712201033\n",
      "200 21.4713768333\n",
      "210 15.0590210191\n",
      "220 10.6000954248\n",
      "230 7.48021631279\n",
      "240 5.29060377226\n",
      "250 3.74961469033\n",
      "260 2.66246697974\n",
      "270 1.8939164217\n",
      "280 1.34934213375\n",
      "290 0.962734090346\n",
      "300 0.68780768503\n",
      "310 0.49199971179\n",
      "320 0.352334194884\n",
      "330 0.252581525497\n",
      "340 0.181248488809\n",
      "350 0.130179976371\n",
      "360 0.093579397641\n",
      "370 0.0673225793493\n",
      "380 0.0484688960547\n",
      "390 0.0349190419537\n",
      "400 0.0251732575668\n",
      "410 0.0181584794304\n",
      "420 0.0131057618799\n",
      "430 0.00946401928291\n",
      "440 0.00683754518776\n",
      "450 0.00494227087858\n",
      "460 0.00357388643825\n",
      "470 0.00258542835387\n",
      "480 0.00187106394385\n",
      "490 0.00135457027746\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ###  Forward pass ###\n",
    "    \n",
    "    # hidden layer\n",
    "    hidden_pre_activation = X.dot(W1)          # 64 * 100\n",
    "    # reLU activation\n",
    "    hidden_activation = np.maximum(hidden_pre_activation, 0) \n",
    "    # ouput                                    # prediction is just linear - no activation applied\n",
    "    y_pred = hidden_activation.dot(W2)         # 64 * 10\n",
    "\n",
    "    ###  Compute and print loss  ###\n",
    "    loss = np.square(y_pred - y).sum()         # squared error loss (RSS)\n",
    "    if epoch % 10 == 0: print(epoch, loss)\n",
    "\n",
    "    ###  Backprop  ###\n",
    "    \n",
    "    # Step 1: output loss\n",
    "    gradient_y_pred = 2.0 * (y_pred - y)       # gradient for squared error, 64 * 10\n",
    "    \n",
    "    # Step 2: loss for W2 / hidden activation\n",
    "    gradient_W2 = hidden_activation.T.dot(gradient_y_pred)             # gradient w.r.t. W2, 100 * 10\n",
    "    gradient_hidden_activation = gradient_y_pred.dot(W2.T)             # gradient w.r.t. hidden activation, 64 * 100\n",
    "    \n",
    "    # step 3: gradient for hidden pre-activation\n",
    "    gradient_hidden_pre_activation = gradient_hidden_activation.copy() # gradient w.r.t. hidden pre-activation\n",
    "    gradient_hidden_pre_activation[hidden_pre_activation < 0] = 0\n",
    "    \n",
    "    # step 4: gradient for W1\n",
    "    gradient_W1 = X.T.dot(gradient_hidden_pre_activation)              # gradient w.r.t. W1, 1000 * 100\n",
    "\n",
    "    ### Update weights ###\n",
    "    W1 -= learning_rate * gradient_W1\n",
    "    W2 -= learning_rate * gradient_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
