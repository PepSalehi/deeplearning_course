{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep neural network with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic behind deep neural nets is backpropagation.\n",
    "To see that there is no rocket science in simple backprop, we are going to build a neural network with NumPy alone, thus implementing backpropagation ourselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define basic dimensions of our network\n",
    "n = 64\n",
    "num_features = 1000\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1000), (64, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input data\n",
    "X = np.random.randn(n, num_features)           # 64 * 1000\n",
    "y = np.random.randn(n, output_dim)             # 64 * 10\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "W1 = np.random.randn(num_features, hidden_dim) # 1000 * 100\n",
    "W2 = np.random.randn(hidden_dim, output_dim)   # 100 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36132714,  1.22576587, -0.05194421, ..., -0.36190339,\n",
       "        -0.43697589, -0.31445682],\n",
       "       [ 1.71998368,  0.85543911,  0.05228614, ..., -0.16534124,\n",
       "        -2.70286056,  0.12855479],\n",
       "       [-0.85330732,  1.17836695,  1.83711836, ..., -0.38646043,\n",
       "        -0.26723985,  0.60119462],\n",
       "       ..., \n",
       "       [ 0.07949523,  0.34050307, -0.06565425, ...,  0.52930321,\n",
       "        -0.93189631,  0.99069479],\n",
       "       [ 0.46378986, -0.53799953, -1.46890349, ..., -0.41189977,\n",
       "        -1.12399145,  0.18397312],\n",
       "       [-1.8530992 , -0.62059888,  0.88532707, ..., -1.08826959,\n",
       "         0.20346044, -0.85422642]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30048037.4452\n",
      "10 896577.934518\n",
      "20 183787.071936\n",
      "30 61764.3442444\n",
      "40 25298.1444104\n",
      "50 11653.5051003\n",
      "60 5780.83821191\n",
      "70 3001.77813303\n",
      "80 1607.61528996\n",
      "90 879.354733926\n",
      "100 488.604966362\n",
      "110 274.881033505\n",
      "120 156.178788661\n",
      "130 89.4464644647\n",
      "140 51.5773915823\n",
      "150 29.9184870772\n",
      "160 17.4474409194\n",
      "170 10.2234272049\n",
      "180 6.01711616324\n",
      "190 3.55595032873\n",
      "200 2.1096265434\n",
      "210 1.25612406174\n",
      "220 0.750535691915\n",
      "230 0.449938832292\n",
      "240 0.270597814219\n",
      "250 0.163238898833\n",
      "260 0.0987686465505\n",
      "270 0.0599320081536\n",
      "280 0.0364683442299\n",
      "290 0.0222510825048\n",
      "300 0.0136125996377\n",
      "310 0.00834901087892\n",
      "320 0.0051336462588\n",
      "330 0.00316430305399\n",
      "340 0.00195499806325\n",
      "350 0.00121066155349\n",
      "360 0.000751412708868\n",
      "370 0.000467363286073\n",
      "380 0.000291308255073\n",
      "390 0.000181933118592\n",
      "400 0.000113846961665\n",
      "410 7.13723269271e-05\n",
      "420 4.48238245967e-05\n",
      "430 2.81993631479e-05\n",
      "440 1.77691888223e-05\n",
      "450 1.12147850687e-05\n",
      "460 7.08854770301e-06\n",
      "470 4.48702256168e-06\n",
      "480 2.84417845298e-06\n",
      "490 1.8051952739e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ###  Forward pass ###\n",
    "    \n",
    "    # hidden layer\n",
    "    hidden_pre_activation = X.dot(W1)          # 64 * 100\n",
    "    # reLU activation\n",
    "    hidden_activation = np.maximum(hidden_pre_activation, 0) \n",
    "    # ouput                                    # prediction is just linear - no activation applied\n",
    "    y_pred = hidden_activation.dot(W2)         # 64 * 10\n",
    "\n",
    "    ###  Compute and print loss  ###\n",
    "    loss = np.square(y_pred - y).sum()         # squared error loss (RSS)\n",
    "    if epoch % 10 == 0: print(epoch, loss)\n",
    "\n",
    "    ###  Backprop  ###\n",
    "    \n",
    "    # Step 1: output loss\n",
    "    gradient_y_pred = 2.0 * (y_pred - y)       # gradient for squared error, 64 * 10\n",
    "    \n",
    "    # Step 2: loss for W2 / hidden activation\n",
    "    gradient_W2 = hidden_activation.T.dot(gradient_y_pred)             # gradient w.r.t. W2, 100 * 10\n",
    "    gradient_hidden_activation = gradient_y_pred.dot(W2.T)             # gradient w.r.t. hidden activation, 64 * 100\n",
    "    \n",
    "    # step 3: gradient for hidden pre-activation\n",
    "    gradient_hidden_pre_activation = gradient_hidden_activation.copy() # gradient w.r.t. hidden pre-activation\n",
    "    gradient_hidden_pre_activation[hidden_pre_activation < 0] = 0\n",
    "    \n",
    "    # step 4: gradient for W1\n",
    "    gradient_W1 = X.T.dot(gradient_hidden_pre_activation)              # gradient w.r.t. W1, 1000 * 100\n",
    "\n",
    "    ### Update weights ###\n",
    "    W1 -= learning_rate * gradient_W1\n",
    "    W2 -= learning_rate * gradient_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
