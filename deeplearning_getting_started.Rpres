<style>
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:100%;
}

.banner {
    position: fixed; 
    top: 0%;
    text-align:right; 
    width:100%;
}

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 1.6em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: .7em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Dive into Deep Learning
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/01/08
autosize: true
incremental:false


Agenda
========================================================

&nbsp;

### Part 1: Getting a feel for deep learning

&nbsp;

- Intro: What is Deep Learning and how does it work?
- Implementing a neural network in NumPy
- Linear regression using DL frameworks - meet Keras, TensorFlow, and PyTorch
- Under the hood: Backpropagation in NumPy vs. TensorFlow vs. PyTorch

<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Agenda
========================================================

&nbsp;

### Part 2: Going deeper with CNNs, LSTMs and hyperparameter tuning

&nbsp;

- Convolutional Neural Networks (CNNs) for image classification
- Long Short Term Memory (LSTM) for sequential data
- Hyperparameter optimization with Keras and its scikit-learn API 


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


About this course 
========================================================


&nbsp;

- Focus is on code and examples (Python)
- All code is made available as Jupyter notebooks
- Main framework used is Keras
- But part 1 also demonstrates examples in TensorFlow and PyTorch, for comparison


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Notebooks
========================================================
class:smalltext

&nbsp;

### Part 1

- Implementing a deep neural network with NumPy: dl_with_numpy.ipynb
- Linear regression, the usual way (using scikit-learn): linear_regression_sklearn.ipynb
- Keras basics: keras_basics.ipynb
- Linear regression with Keras: linear_regression_keras.ipynb
- TensorFlow basics: tensorflow_basics.ipynb
- Linear regression with TensorFlow: linear_regression_tensorflow.ipynb
- PyTorch basics: pytorch_basics.ipynb
- Linear regression with PyTorch: linear_regression_pytorch.ipynb
- Backpropagation in NumPy vs TensorFlow vs PyTorch: backprop_numpy_tf_pytorch.ipynb


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Notebooks
========================================================
class:smalltext


&nbsp;

### Part 2

- Convolutional Neural Networks with Keras (1): cnn_keras.ipynb
- Convolutional Neural Networks with Keras (2): cnn_keras_pretrained.ipynb
- Convolutional Neural Networks with Keras (3): cnn_keras_pretrained_2.ipynb
- Long Short Term Memory (LSTM) with Keras (1): lstm_keras_1.ipynb
- Long Short Term Memory (LSTM) with Keras (2): lstm_keras_2.ipynb
- Hyperparameter optimization using Keras and the scikit-learn API: optimization.ipynb


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Why Deep Learning?</h1>


Easy? Difficult?
========================================================

&nbsp;

- playing chess
- solving matrix computations

- recognizing objects
- understanding speech
- talking
- walking
- driving


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Easy? Difficult?
========================================================


&nbsp;

What's difficult for us is easy for the machine,- and vice versa.


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Representations matter
========================================================


&nbsp;

<figure>
    <img src='coords.png' width='60%'/>
    <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a></figcaption>
</figure>


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Just feed the network the right features?
========================================================


&nbsp;

What are the correct pixel values for a "bike" feature?

- race bike, mountain bike, e-bike?
- pixels in the shadow may be much darker
- what if bike is mostly obscured by rider standing in front?


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Let the network pick the features
========================================================


&nbsp;

... a layer at a time!

<figure>
    <img src='features.png' width='50%'/> 
    <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a></figcaption>

</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Deep Learning, two ways to think about it
========================================================


&nbsp;


- hierarchical feature extraction (start simple, end complex)
- function composition (see http://colah.github.io/posts/2015-09-NN-Types-FP/)

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>A short history of deep learning</h1>



The first wave: cybernetics (1940s - 1960s)
========================================================


&nbsp;

- neuroscientific motivation
- linear models


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



McCulloch-Pitts Neuron (MCP, 1943, a.k.a. Logic Circuit)
========================================================
class:smalltext

&nbsp;

- binary output (0 or 1)
- neurons may have inhibiting (negative) and excitatory (positive) inputs
- each neuron has a threshold that has to be surpassed by the sum of activations for the neuron to get active (output 1)
- if just one input is inhibitory, the neuron will not activate

<figure>
    <img src='mcp.png' width='10%'/>
    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>
</figure>


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>




Perceptron (Rosenblatt, 1958): Great expectations
========================================================


&nbsp;

- compute linear combination of inputs
- return +1 if result is positive, -1 if result is negative

<figure>
    <img src='perceptron.png' width='60%'/>
    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Minsky & Papert (1969), "Perceptrons": the great disappointment
========================================================


&nbsp;

- Perceptrons can only solve linearly separable problems
- Big loss of interest in neural networks

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



The second wave: Connectionism (1980s, mid-1990s)
========================================================


&nbsp;

- distributed representations
- backpropagation 

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Backpropagation, the magic ingredient
========================================================
class:smalltext

&nbsp;

Several "origins" in different fields, see e.g.

- Henry J. Kelley (1960). Gradient theory of optimal flight paths. Ars Journal, 30(10), 947-954.

- Arthur E. Bryson (1961, April). A gradient method for optimizing multi-stage allocation processes. In Proceedings of the Harvard Univ. Symposium on digital computers and their applications.

- Paul Werbos (1974). Beyond regression: New tools for prediction and analysis in the behavioral sciences. PhD thesis, Harvard University.

- Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (8 October 1986). "Learning representations by back-propagating errors". Nature. 323 (6088): 533â€“536.

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Back at the time: How could the magic fail?
========================================================


&nbsp;

- Only applicable in case of supervised learning
- Doesn't scale well to multiple layers (or so they thought at the time)
- Can converge to poor local minima (or so they thought at the time)

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


The third wave: Deep Learning
========================================================


&nbsp;

- It all starts with: Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554.
- Deep neural networks can be trained efficiently, if the weights are initialized intelligently
- Return of backpropagation

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


The architectures en vogue now (CNN, LSTM...) have mostly been around since the 1980s/1990s.
========================================================


&nbsp;

So why the hype, ehm, success now?

- Big Data
- big models (due to better hardware, mostly)
- big incentives (deep learning is profitable!)

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>How does a deep network learn?</h1>



Feedforward Deep Neural Network
========================================================

<figure>
    <img src='deep_nn.png' width='60%' />
    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>
</figure>


Why hidden layers? Learning XOR
========================================================
class:smalltext

&nbsp;


We want to predict
- 0 from [0,0]
- 0 from [1,1]
- 1 from [0,1]
- 1 from [1,0]


<figure>
    <img src='xor1.png' width='60%'/>
    <figcaption></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Trying a linear model
========================================================


&nbsp;

$f(\mathbf{x}; \mathbf{w}, b) = \mathbf{x}^T\mathbf{w} + b$    


- with Mean Squared Error cost (MSE), this leads to: $\mathbf{w}=0, b=0.5$
- mapping every point to 0.5!


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Introduce hidden layer
========================================================


&nbsp;

$f(\mathbf{x}; \mathbf{W}, \mathbf{c}, \mathbf{w}, b) = \mathbf{w}^T (\mathbf{W}^T\mathbf{x} + \mathbf{c}) + b$

<figure>
    <img src='xor4.png' width='40%'/>
    <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Calculation with hidden layer (1)
========================================================
class:small-code

&nbsp;


```{r}
(X <- matrix(c(0,0,0,1,1,0,1,1), nrow = 4, ncol = 2, byrow = TRUE))
(W <- matrix(c(0,0,0,0,0,0,0,1,0,0,0,1,0,1,1,1), nrow = 4, ncol = 4, byrow = TRUE))
(c <- matrix(rep(c(0,-1),4), nrow = 4, byrow = TRUE))
``` 


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Calculation with hidden layer (2)
========================================================
class:small-code

&nbsp;


```{r}
(matmul <- t(W) %*% X)
(hidden <- matmul + c)
``` 


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Which gives us...
========================================================
class:small-code

&nbsp;

```{r, echo=FALSE}
library(ggplot2)
library(dplyr)
hidden <- data.frame(cbind(hidden,c(0,1,1,0))) %>% mutate(CLASS = factor(X3))
ggplot(hidden, aes(X1,X2)) + geom_point(aes(color=CLASS))

``` 


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Introducing nonlinearity
========================================================
class:small-code

&nbsp;


$f(\mathbf{x}; \mathbf{W}, \mathbf{c}, \mathbf{w}, b) = \mathbf{w}^T max(0, \mathbf{W}^T\mathbf{x} + \mathbf{c}) + b$


&nbsp;

```{r,echo=FALSE}
hidden_relu <- hidden %>% mutate(X1 = ifelse(X1 > 0, X1, 0), X2 = ifelse(X2 > 0, X2, 0)) %>% select(-X3)
``` 

```{r}
hidden_relu 
``` 


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Introducing nonlinearity
========================================================


&nbsp;

```{r,echo=FALSE}
ggplot(hidden_relu, aes(X1,X2)) + geom_point(aes(color=CLASS), size=6)

``` 


The remaining hidden-to-output transformation is linear.

But the classes are already linearly separable.

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



Optimization
========================================================


&nbsp;

- Like other machine learning algorithms, neural networks learn by _minimizing a cost function_.
- Cost functions in neural networks normally are not convex and so, cannot be optimized in closed form.
- The solution is to do gradient descent.

<figure>
    <img src='convex.png' width='40%'/>
     <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Local minima
========================================================

&nbsp;

<figure>
    <img src='local_minima.png' width='60%'/>
     <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a>
</figure>


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Closed-form vs. gradient descent optimization for Least Squares
========================================================

&nbsp;

- Minimize squared error $f(\mathbf{x}) = ||\mathbf{X\hat{\beta} - y}||^2_2$
- Closed form: solve __normal equations__ $\mathbf{\hat{\beta}} = (\mathbf{X}^T\mathbf{{X}})^{-1} \mathbf{X}^T \mathbf{y}$
- Alternatively, follow the gradient: $\nabla_x f(\mathbf{x})= \mathbf{X}^T\mathbf{X}\mathbf{\hat{\beta}} - \mathbf{X^Ty}$

<figure>
    <img src='gradient_descent_LS.png' width='50%'/>
     <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a>
</figure>


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


This gives us a way to train one weight matrix.
========================================================

&nbsp;

How about a net with several layers?

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Backpropagation
========================================================

&nbsp;

- basically, just the chain rule: $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$
- chained over several layers:
&nbsp;
<figure>
    <img src='backprop2.png' width='60%'/>
    <figcaption>Source: <a href=https://colah.github.io/posts/2015-08-Backprop/>https://colah.github.io/posts/2015-08-Backprop/</a></figcaption>
</figure>


<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Backprop example: logistic neuron
========================================================


&nbsp;

<figure>
    <img src='backprop3.png' width='60%'/>
    <figcaption>Source: Geoffrey Hinton, <a href='http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec3.pdf'>Neural Networks for Machine Learning Lec. 3</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Decisions (1): Which loss function should I choose?
========================================================
class:smalltext

&nbsp;

The _loss_ (or _cost_) function indicates the cost incurred from false prediction / misclassification

- probably the best-known loss function in machine learning is __mean squared error__: 

  $\frac{1}{n} \sum_n{(\hat{y} - y)^2}$
  
- most of the time, in deep learning classification tasks we use __cross entropy__:

  $- \sum_j{t_j log(y_j)}$
  
  This is the negative log probability of the right answer.
 
<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Decisions (2): Which activation function to choose?
========================================================
class:smalltext

&nbsp;

- for a long time, the sigmoid (logistic) activation function was used a lot: $y = \frac{1}{1 + e^{-z}}$
  
<figure>
<img src='sigmoid.png' width='20%'/>
</figure>
  
  
- now _rectified linear units_ (ReLUs) are preferred: $y = max(0, z)$
  
<figure>
<img src='ReLU.png' width='20%'/>
</figure>
 

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Deep Learning Architectures</h1>



Convolutional Neural Networks
========================================================
class:smalltext

&nbsp;

- standard feedforward networks need equally sized input (images often aren't!)
- convolution operation extracts image features, independently of location

<figure>
    <img src='convnet.jpeg' width="60%" />
     <figcaption>Source: <a href='http://cs231n.github.io/convolutional-networks/'>http://cs231n.github.io/convolutional-networks/</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


The Convolution Operation
========================================================

&nbsp;

<figure>
    <img src='convolution_demo.png' width='60%'/>
    <figcaption>Source: <a href='http://cs231n.github.io/convolutional-networks/'>http://cs231n.github.io/convolutional-networks/</a> (Live Demo on website!)</figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>
 

Play around in Gimp
========================================================

&nbsp;

Playing around with the convolution matrix filter in Gimp is a great way to gain intuition.

<figure>
    <img src='gimp.png' width='60%'/>
    <figcaption>Source: <a href='https://docs.gimp.org/en/plug-in-convmatrix.html'>https://docs.gimp.org/en/plug-in-convmatrix.html</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Recurrent neural networks (RNNs)
========================================================
class:smalltext

&nbsp;

Why add recursion?

- cannot process sequential data with "normal" feedforward networks
- in NLP, the n-gram approach cannot handle long-term relationships

> Jane walked into the room. John walked in too. It was late in the
day, and everyone was walking home after a long day at work. Jane said hi to ___ 

(see [Stanford CS 224D Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf))

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Two representations of RNNs
========================================================

&nbsp;

<figure>
    <img src='rnn1.png' width='60%'/>
    <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Long Short Term Memory (LSTM): The need to forget
========================================================

&nbsp;

<figure>
    <img src='lstm.png' align='left' width='60%'/> <img src='lstm2.png' align='right' width='40%'/>
    <figcaption>Source: <a href='http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf'>Stanford CS 224D Deep Learning for NLP Lecture Notes</a></figcaption>
</figure>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>End of theory - time for practice!</h1>

